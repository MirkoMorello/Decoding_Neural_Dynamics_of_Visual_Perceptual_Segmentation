{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from imageio.v3 import imread, imwrite\n",
    "from PIL import Image\n",
    "import pysaliency\n",
    "from pysaliency.baseline_utils import BaselineModel, CrossvalidatedBaselineModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from deepgaze_pytorch.layers import (\n",
    "    Conv2dMultiInput,\n",
    "    LayerNorm,\n",
    "    LayerNormMultiInput,\n",
    "    Bias,\n",
    "    FlexibleScanpathHistoryEncoding\n",
    ")\n",
    "\n",
    "from deepgaze_pytorch.modules import DeepGazeIII, FeatureExtractor\n",
    "from deepgaze_pytorch.features.densenet import RGBDenseNet201\n",
    "from deepgaze_pytorch.data import ImageDataset, ImageDatasetSampler, FixationDataset, FixationMaskTransform\n",
    "from deepgaze_pytorch.training import _train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_saliency_network(input_channels):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('layernorm0', LayerNorm(input_channels)),\n",
    "        ('conv0', nn.Conv2d(input_channels, 8, (1, 1), bias=False)),\n",
    "        ('bias0', Bias(8)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(8)),\n",
    "        ('conv1', nn.Conv2d(8, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "\n",
    "        ('layernorm2', LayerNorm(16)),\n",
    "        ('conv2', nn.Conv2d(16, 1, (1, 1), bias=False)),\n",
    "        ('bias2', Bias(1)),\n",
    "        ('softplus2', nn.Softplus()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def build_scanpath_network():\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('encoding0', FlexibleScanpathHistoryEncoding(in_fixations=4, channels_per_fixation=3, out_channels=128, kernel_size=[1, 1], bias=True)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(128)),\n",
    "        ('conv1', nn.Conv2d(128, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def build_fixation_selection_network(scanpath_features=16):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('layernorm0', LayerNormMultiInput([1, scanpath_features])),\n",
    "        ('conv0', Conv2dMultiInput([1, scanpath_features], 128, (1, 1), bias=False)),\n",
    "        ('bias0', Bias(128)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(128)),\n",
    "        ('conv1', nn.Conv2d(128, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "\n",
    "        ('conv2', nn.Conv2d(16, 1, (1, 1), bias=False)),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spatial_dataset(stimuli, fixations, centerbias, batch_size, path=None):\n",
    "    if path is not None:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        lmdb_path = str(path)\n",
    "    else:\n",
    "        lmdb_path = None\n",
    "\n",
    "    dataset = ImageDataset(\n",
    "        stimuli=stimuli,\n",
    "        fixations=fixations,\n",
    "        centerbias_model=centerbias,\n",
    "        transform=FixationMaskTransform(sparse=False),\n",
    "        average='image',\n",
    "        lmdb_path=lmdb_path,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=ImageDatasetSampler(dataset, batch_size=batch_size),\n",
    "        pin_memory=True,\n",
    "        num_workers=os.cpu_count(),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_scanpath_dataset(stimuli, fixations, centerbias, batch_size, path=None):\n",
    "    if path is not None:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        lmdb_path = str(path)\n",
    "    else:\n",
    "        lmdb_path = None\n",
    "\n",
    "    dataset = FixationDataset(\n",
    "        stimuli=stimuli,\n",
    "        fixations=fixations,\n",
    "        centerbias_model=centerbias,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "        allow_missing_fixations=True,\n",
    "        transform=FixationMaskTransform(sparse=False),\n",
    "        average='image',\n",
    "        lmdb_path=lmdb_path,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=ImageDatasetSampler(dataset, batch_size=batch_size),\n",
    "        pin_memory=True,\n",
    "        num_workers=os.cpu_count(),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = Path('pysaliency_datasets')\n",
    "train_directory = Path('train_deepgaze3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Using GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on SALICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SALICON data from pysaliency_datasets...\n",
      "SALICON data loaded.\n",
      "Initializing BaselineModel...\n",
      "BaselineModel initialized.\n",
      "Loaded cached train baseline log likelihood from: pysaliency_datasets/salicon_baseline_train_ll.pkl\n",
      "Loaded cached validation baseline log likelihood from: pysaliency_datasets/salicon_baseline_val_ll.pkl\n",
      "------------------------------\n",
      "Final Train Baseline Log Likelihood: 0.46408017115279726\n",
      "Final Validation Baseline Log Likelihood: 0.4291592320821601\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# Assume dataset_directory is defined appropriately, e.g.:\n",
    "# dataset_directory = '/path/to/your/datasets'\n",
    "# Ensure the directory exists or create it if it doesn't\n",
    "# os.makedirs(dataset_directory, exist_ok=True) # Might be needed if dataset_directory is just for caching\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading SALICON data from {dataset_directory}...\")\n",
    "SALICON_train_stimuli, SALICON_train_fixations = pysaliency.get_SALICON_train(location=dataset_directory)\n",
    "SALICON_val_stimuli, SALICON_val_fixations = pysaliency.get_SALICON_val(location=dataset_directory)\n",
    "print(\"SALICON data loaded.\")\n",
    "\n",
    "# --- Define Model ---\n",
    "# parameters taken from an early fit for MIT1003. Since SALICON has many more fixations, the bandwidth won't be too small\n",
    "print(\"Initializing BaselineModel...\")\n",
    "SALICON_centerbias = BaselineModel(stimuli=SALICON_train_stimuli, fixations=SALICON_train_fixations, bandwidth=0.0217, eps=2e-13, caching=False)\n",
    "print(\"BaselineModel initialized.\")\n",
    "\n",
    "# --- Define cache file paths ---\n",
    "train_ll_cache_file = os.path.join(dataset_directory, 'salicon_baseline_train_ll.pkl')\n",
    "val_ll_cache_file = os.path.join(dataset_directory, 'salicon_baseline_val_ll.pkl')\n",
    "\n",
    "# --- Compute or Load Train Baseline Log Likelihood ---\n",
    "try:\n",
    "    # Attempt to load from cache\n",
    "    with open(train_ll_cache_file, 'rb') as f:\n",
    "        train_baseline_log_likelihood = pickle.load(f)\n",
    "    print(f\"Loaded cached train baseline log likelihood from: {train_ll_cache_file}\")\n",
    "except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "    # Compute if cache doesn't exist or is invalid\n",
    "    print(f\"Cache not found or invalid ({e}). Computing train baseline log likelihood...\")\n",
    "    train_baseline_log_likelihood = SALICON_centerbias.information_gain(\n",
    "        SALICON_train_stimuli,\n",
    "        SALICON_train_fixations,\n",
    "        verbose=True,\n",
    "        average='image'\n",
    "    )\n",
    "    print(f\"Computation finished. Train LL = {train_baseline_log_likelihood}\")\n",
    "    # Save the result\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(train_ll_cache_file), exist_ok=True) # Ensure directory exists\n",
    "        with open(train_ll_cache_file, 'wb') as f:\n",
    "            pickle.dump(train_baseline_log_likelihood, f)\n",
    "        print(f\"Saved train baseline log likelihood to: {train_ll_cache_file}\")\n",
    "    except Exception as save_e:\n",
    "        print(f\"Error saving cache file {train_ll_cache_file}: {save_e}\")\n",
    "\n",
    "\n",
    "# --- Compute or Load Validation Baseline Log Likelihood ---\n",
    "try:\n",
    "    # Attempt to load from cache\n",
    "    with open(val_ll_cache_file, 'rb') as f:\n",
    "        val_baseline_log_likelihood = pickle.load(f)\n",
    "    print(f\"Loaded cached validation baseline log likelihood from: {val_ll_cache_file}\")\n",
    "except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "    # Compute if cache doesn't exist or is invalid\n",
    "    print(f\"Cache not found or invalid ({e}). Computing validation baseline log likelihood...\")\n",
    "    val_baseline_log_likelihood = SALICON_centerbias.information_gain(\n",
    "        SALICON_val_stimuli,\n",
    "        SALICON_val_fixations,\n",
    "        verbose=True,\n",
    "        average='image'\n",
    "    )\n",
    "    print(f\"Computation finished. Validation LL = {val_baseline_log_likelihood}\")\n",
    "    # Save the result\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(val_ll_cache_file), exist_ok=True) # Ensure directory exists\n",
    "        with open(val_ll_cache_file, 'wb') as f:\n",
    "            pickle.dump(val_baseline_log_likelihood, f)\n",
    "        print(f\"Saved validation baseline log likelihood to: {val_ll_cache_file}\")\n",
    "    except Exception as save_e:\n",
    "        print(f\"Error saving cache file {val_ll_cache_file}: {save_e}\")\n",
    "\n",
    "\n",
    "# --- Final Output ---\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Train Baseline Log Likelihood: {train_baseline_log_likelihood}\")\n",
    "print(f\"Final Validation Baseline Log Likelihood: {val_baseline_log_likelihood}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mirko/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = DeepGazeIII(\n",
    "    features=FeatureExtractor(RGBDenseNet201(), [\n",
    "            '1.features.denseblock4.denselayer32.norm1',\n",
    "            '1.features.denseblock4.denselayer32.conv1',\n",
    "            '1.features.denseblock4.denselayer31.conv2',\n",
    "        ]),\n",
    "    saliency_network=build_saliency_network(2048),\n",
    "    scanpath_network=None,\n",
    "    fixation_selection_network=build_fixation_selection_network(scanpath_features=0),\n",
    "    downsample=1.5,\n",
    "    readout_factor=4,\n",
    "    saliency_map_factor=4,\n",
    "    included_fixations=[],\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30, 45, 60, 75, 90, 105, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid LMDB found at train_deepgaze3/lmdb_cache/SALICON_train with 10000 items. Skipping generation.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68992355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 68992354/68992355 [00:16<00:00, 4311795.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid LMDB found at train_deepgaze3/lmdb_cache/SALICON_val with 5000 items. Skipping generation.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 38846997/38846998 [00:08<00:00, 4471019.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_spatial_dataset(SALICON_train_stimuli, SALICON_train_fixations, SALICON_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / 'SALICON_train')\n",
    "validation_loader = prepare_spatial_dataset(SALICON_val_stimuli, SALICON_val_fixations, SALICON_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / 'SALICON_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "validation metrics ['LL', 'IG', 'NSS']\n",
      "Found old checkpoint train_deepgaze3/pretraining/step-0004.pth\n",
      "Restoring from train_deepgaze3/pretraining/step-0004.pth\n",
      "Setting step to 4\n",
      "Continuing from step 4\n",
      "Epoch not yet evaluated, evaluating...\n",
      "DEBUG: Evaluating metrics: ['LL', 'IG', 'NSS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/157 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 505910, 505914, 505918, 505919, 505923, 505924, 505928, 505929, 505937, 505938, 505942, 505943) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/queue.py:179\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    180\u001b[39m \u001b[38;5;28mself\u001b[39m.not_empty.wait(remaining)\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpretraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_baseline_log_likelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_baseline_log_likelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminimum_learning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mIG\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNSS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/DeepGaze/deepgaze_pytorch/training.py:427\u001b[39m, in \u001b[36m_train\u001b[39m\u001b[34m(this_directory, model, train_loader, train_baseline_log_likelihood, val_loader, val_baseline_log_likelihood, optimizer, lr_scheduler, minimum_learning_rate, validation_metric, validation_metrics, validation_epochs, startwith, device)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m progress.epoch.values:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEpoch not yet evaluated, evaluating...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[43msave_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# We have to make one scheduler step here, since we make the\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# scheduler step _after_ saving the checkpoint\u001b[39;00m\n\u001b[32m    431\u001b[39m lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/DeepGaze/deepgaze_pytorch/training.py:369\u001b[39m, in \u001b[36m_train.<locals>.save_step\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    366\u001b[39m writer.add_scalar(\u001b[33m'\u001b[39m\u001b[33mparameters/center_bias_weight\u001b[39m\u001b[33m'\u001b[39m, model.finalizer.center_bias_weight.detach().cpu().numpy()[\u001b[32m0\u001b[39m], step)\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % validation_epochs == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     _val_metrics = \u001b[43meval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_baseline_log_likelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    371\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSkipping validation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/DeepGaze/deepgaze_pytorch/training.py:93\u001b[39m, in \u001b[36meval_epoch\u001b[39m\u001b[34m(model, dataset, baseline_information_gain, device, metrics)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     92\u001b[39m     pbar = tqdm(dataset, desc=\u001b[33m\"\u001b[39m\u001b[33mValidating\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# --- Move batch to GPU ---\u001b[39;49;00m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcenterbias\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcenterbias\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/utils/data/dataloader.py:488\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    486\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28mself\u001b[39m._get_iterator()\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1230\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._reset\u001b[39m\u001b[34m(self, loader, first_iter)\u001b[39m\n\u001b[32m   1228\u001b[39m resume_iteration_cnt = \u001b[38;5;28mself\u001b[39m._num_workers\n\u001b[32m   1229\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1230\u001b[39m     return_idx, return_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils.worker._ResumeIteration):\n\u001b[32m   1232\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1264\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1263\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1265\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1266\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 505910, 505914, 505918, 505919, 505923, 505924, 505928, 505929, 505937, 505938, 505942, 505943) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "_train(train_directory / 'pretraining',\n",
    "    model,\n",
    "    train_loader, train_baseline_log_likelihood,\n",
    "    validation_loader, val_baseline_log_likelihood,\n",
    "    optimizer, lr_scheduler,\n",
    "    minimum_learning_rate=1e-7,\n",
    "    device=device,\n",
    "    validation_metrics=['LL', 'IG', 'NSS'],\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the MIT1003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_stimuli_orig, mit_scanpaths_orig = pysaliency.external_datasets.mit.get_mit1003_with_initial_fixation(location=dataset_directory, replace_initial_invalid_fixations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15045/15045 [00:00<00:00, 608042.93it/s]\n",
      "/tmp/ipykernel_478814/2048604840.py:54: UserWarning: don't use attributes for FixationTrains, use scanpath_attributes or scanpath_fixation_attributes instead!\n",
      "  new_fixations = pysaliency.FixationTrains(\n",
      "100%|██████████| 1003/1003 [00:04<00:00, 217.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_stimulus(input_image):\n",
    "    size = input_image.shape[0], input_image.shape[1]\n",
    "    if size[0] < size[1]:\n",
    "        new_size = 768, 1024\n",
    "    else:\n",
    "        new_size = 1024,768\n",
    "    \n",
    "    # pillow uses width, height\n",
    "    new_size = tuple(list(new_size)[::-1])\n",
    "    \n",
    "    new_stimulus = np.array(Image.fromarray(input_image).resize(new_size, Image.BILINEAR))\n",
    "    return new_stimulus\n",
    "\n",
    "def convert_fixations(stimuli, fixations):\n",
    "    new_fixations = fixations.copy()\n",
    "    for n in tqdm(list(range(len(stimuli)))):\n",
    "        stimulus = stimuli.stimuli[n]\n",
    "        size = stimulus.shape[0], stimulus.shape[1]\n",
    "        if size[0] < size[1]:\n",
    "            new_size = 768, 1024\n",
    "        else:\n",
    "            new_size = 1024,768\n",
    "        x_factor = new_size[1] / size[1]\n",
    "        y_factor = new_size[0] / size[0]\n",
    "        \n",
    "        inds = new_fixations.n == n\n",
    "        new_fixations.x[inds] *= x_factor\n",
    "        new_fixations.y[inds] *= y_factor\n",
    "        new_fixations.x_hist[inds] *= x_factor\n",
    "        new_fixations.y_hist[inds] *= y_factor\n",
    "    \n",
    "    return new_fixations\n",
    "\n",
    "def convert_fixation_trains(stimuli, fixations):\n",
    "    train_xs = fixations.train_xs.copy()\n",
    "    train_ys = fixations.train_ys.copy()\n",
    "    \n",
    "    for i in tqdm(range(len(train_xs))):\n",
    "        n = fixations.train_ns[i]\n",
    "        \n",
    "        size = stimuli.shapes[n][0], stimuli.shapes[n][1]\n",
    "        \n",
    "        if size[0] < size[1]:\n",
    "            new_size = 768, 1024\n",
    "        else:\n",
    "            new_size = 1024,768\n",
    "        \n",
    "        x_factor = new_size[1] / size[1]\n",
    "        y_factor = new_size[0] / size[0]\n",
    "        \n",
    "        train_xs[i] *= x_factor\n",
    "        train_ys[i] *= y_factor\n",
    "        \n",
    "    new_fixations = pysaliency.FixationTrains(\n",
    "        train_xs = train_xs,\n",
    "        train_ys = train_ys,\n",
    "        train_ts = fixations.train_ts.copy(),\n",
    "        train_ns = fixations.train_ns.copy(),\n",
    "        train_subjects = fixations.train_subjects.copy(),\n",
    "        attributes={key: getattr(fixations, key).copy() for key in fixations.__attributes__ if key not in ['subjects', 'scanpath_index']},\n",
    "    )\n",
    "    return new_fixations\n",
    "\n",
    "\n",
    "\n",
    "def convert_stimuli(stimuli, new_location: Path):\n",
    "    assert isinstance(stimuli, pysaliency.FileStimuli)\n",
    "    new_stimuli_location = new_location / 'stimuli'\n",
    "    new_stimuli_location.mkdir(parents=True, exist_ok=True)\n",
    "    new_filenames = []\n",
    "    for filename in tqdm(stimuli.filenames):\n",
    "        stimulus = imread(filename)\n",
    "        new_stimulus = convert_stimulus(stimulus)\n",
    "        \n",
    "        basename = os.path.basename(filename)\n",
    "        new_filename = new_stimuli_location / basename\n",
    "        if new_stimulus.size != stimulus.size:\n",
    "            imwrite(new_filename, new_stimulus)\n",
    "        else:\n",
    "            #print(\"Keeping\")\n",
    "            shutil.copy(filename, new_filename)\n",
    "        new_filenames.append(new_filename)\n",
    "    return pysaliency.FileStimuli(new_filenames)\n",
    "\n",
    "mit_scanpaths_twosize = convert_fixation_trains(mit_stimuli_orig, mit_scanpaths_orig)\n",
    "mit_stimuli_twosize = convert_stimuli(mit_stimuli_orig, train_directory / 'MIT1003_twosize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the initial forced fixation from the training data, it's only used for conditioning\n",
    "mit_fixations_twosize = mit_scanpaths_twosize[mit_scanpaths_twosize.lengths > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters optimized on MIT1003 for maximum leave-one-image-out crossvalidation log-likelihood\n",
    "MIT1003_centerbias = CrossvalidatedBaselineModel(\n",
    "    mit_stimuli_twosize,\n",
    "    mit_fixations_twosize,\n",
    "    bandwidth=10**-1.6667673342543432,\n",
    "    eps=10**-14.884189168516073,\n",
    "    caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random shuffles for crossvalidation\n",
      "Using random shuffles for crossvalidation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/808 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 95/808 [00:09<01:13,  9.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m MIT1003_stimuli_train, MIT1003_fixations_train = pysaliency.dataset_config.train_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=\u001b[32m10\u001b[39m, fold_no=crossval_fold)\n\u001b[32m      3\u001b[39m MIT1003_stimuli_val, MIT1003_fixations_val = pysaliency.dataset_config.validation_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=\u001b[32m10\u001b[39m, fold_no=crossval_fold)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_baseline_log_likelihood = \u001b[43mMIT1003_centerbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43minformation_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMIT1003_stimuli_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMIT1003_fixations_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m val_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_val, MIT1003_fixations_val, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, average=\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# finetune spatial model on MIT1003\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/pysaliency/models.py:177\u001b[39m, in \u001b[36mScanpathModel.information_gain\u001b[39m\u001b[34m(self, stimuli, fixations, baseline_model, verbose, average)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minformation_gain\u001b[39m(\u001b[38;5;28mself\u001b[39m, stimuli, fixations, baseline_model=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m, average=\u001b[33m'\u001b[39m\u001b[33mfixation\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m average_values(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minformation_gains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstimuli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m, fixations, average=average)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/pysaliency/models.py:172\u001b[39m, in \u001b[36mScanpathModel.information_gains\u001b[39m\u001b[34m(self, stimuli, fixations, baseline_model, verbose, average)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m baseline_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     baseline_model = UniformModel()\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m own_log_likelihoods = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihoods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstimuli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m baseline_log_likelihoods = baseline_model.log_likelihoods(stimuli, fixations, verbose=verbose)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (own_log_likelihoods - baseline_log_likelihoods) / np.log(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/pysaliency/models.py:335\u001b[39m, in \u001b[36mModel.log_likelihoods\u001b[39m\u001b[34m(self, stimuli, fixations, verbose)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inds.sum():\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m log_density = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstimuli\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m check_prediction_shape(log_density, stimuli[n])\n\u001b[32m    337\u001b[39m this_log_likelihoods = log_density[fixations.y_int[inds], fixations.x_int[inds]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/pysaliency/models.py:310\u001b[39m, in \u001b[36mModel.log_density\u001b[39m\u001b[34m(self, stimulus)\u001b[39m\n\u001b[32m    308\u001b[39m stimulus = handle_stimulus(stimulus)\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.caching:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstimulus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstimulus_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m stimulus_id = stimulus.stimulus_id\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stimulus_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/pysaliency/baseline_utils.py:582\u001b[39m, in \u001b[36mCrossvalidatedBaselineModel._log_density\u001b[39m\u001b[34m(self, stimulus)\u001b[39m\n\u001b[32m    580\u001b[39m _fixations = np.array([\u001b[38;5;28mself\u001b[39m.ys[inds]*shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.xs[inds]*shape[\u001b[32m1\u001b[39m]]).T\n\u001b[32m    581\u001b[39m fill_fixation_map(ZZ, _fixations)\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m ZZ = \u001b[43mgaussian_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m*\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m*\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m ZZ *= (\u001b[32m1\u001b[39m-\u001b[38;5;28mself\u001b[39m.eps)\n\u001b[32m    584\u001b[39m ZZ += \u001b[38;5;28mself\u001b[39m.eps * \u001b[32m1.0\u001b[39m/(shape[\u001b[32m0\u001b[39m]*shape[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/scipy/ndimage/_filters.py:425\u001b[39m, in \u001b[36mgaussian_filter\u001b[39m\u001b[34m(input, sigma, order, output, mode, cval, truncate, radius, axes)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(axes) > \u001b[32m0\u001b[39m:\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m axis, sigma, order, mode, radius \u001b[38;5;129;01min\u001b[39;00m axes:\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m         \u001b[43mgaussian_filter1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m=\u001b[49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m         \u001b[38;5;28minput\u001b[39m = output\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/scipy/ndimage/_filters.py:323\u001b[39m, in \u001b[36mgaussian_filter1d\u001b[39m\u001b[34m(input, sigma, axis, order, output, mode, cval, truncate, radius)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Since we are calling correlate, not convolve, revert the kernel\u001b[39;00m\n\u001b[32m    322\u001b[39m weights = _gaussian_kernel1d(sigma, order, lw)[::-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/scipy/ndimage/_filters.py:180\u001b[39m, in \u001b[36mcorrelate1d\u001b[39m\u001b[34m(input, weights, axis, output, mode, cval, origin)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mInvalid origin; origin must satisfy \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    177\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33m-(len(weights) // 2) <= origin <= \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    178\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33m(len(weights)-1) // 2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    179\u001b[39m mode = _ni_support._extend_mode_to_code(mode)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43m_nd_image\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m                      \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for crossval_fold in range(10):\n",
    "    MIT1003_stimuli_train, MIT1003_fixations_train = pysaliency.dataset_config.train_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=10, fold_no=crossval_fold)\n",
    "    MIT1003_stimuli_val, MIT1003_fixations_val = pysaliency.dataset_config.validation_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=10, fold_no=crossval_fold)\n",
    "\n",
    "    train_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_train, MIT1003_fixations_train, verbose=True, average='image')\n",
    "    val_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_val, MIT1003_fixations_val, verbose=True, average='image')\n",
    "\n",
    "    # finetune spatial model on MIT1003\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=None,\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=0),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[],\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18, 21, 24])\n",
    "\n",
    "    train_loader = prepare_spatial_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / f'MIT1003_train_spatial_{crossval_fold}')\n",
    "    validation_loader = prepare_spatial_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / f'MIT1003_val_spatial_{crossval_fold}')\n",
    "\n",
    "    _train(train_directory / 'MIT1003_spatial' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'pretraining' / 'final.pth',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Train scanpath model\n",
    "\n",
    "    train_loader = prepare_scanpath_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / f'MIT1003_train_scanpath_{crossval_fold}')\n",
    "    validation_loader = prepare_scanpath_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / f'MIT1003_val_scanpath_{crossval_fold}')\n",
    "\n",
    "    # first train with partially frozen saliency network\n",
    "\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=build_scanpath_network(),\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=16),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    frozen_scopes = [\n",
    "        \"saliency_network.layernorm0\",\n",
    "        \"saliency_network.conv0\",\n",
    "        \"saliency_network.bias0\",\n",
    "        \"saliency_network.layernorm1\",\n",
    "        \"saliency_network.conv1\",\n",
    "        \"saliency_network.bias1\",\n",
    "    ]\n",
    "\n",
    "    for scope in frozen_scopes:\n",
    "        for parameter_name, parameter in model.named_parameters():\n",
    "            if parameter_name.startswith(scope):\n",
    "                print(\"Fixating parameter\", parameter_name)\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 31, 32, 33, 34, 35])\n",
    "\n",
    "    _train(train_directory / 'MIT1003_scanpath_partially_frozen_saliency_network' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'MIT1003_spatial' /  f'crossval-10-{crossval_fold}' / 'final.pth'\n",
    "    )\n",
    "\n",
    "    # Now finetune full scanpath model\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=build_scanpath_network(),\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=16),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18, 21, 24])\n",
    "\n",
    "    _train(train_directory / 'MIT1003_scanpath' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'MIT1003_scanpath_partially_frozen_saliency_network' / f'crossval-10-{crossval_fold}' / 'final.pth',\n",
    "        validation_metrics=['LL', 'IG', 'NSS']\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
