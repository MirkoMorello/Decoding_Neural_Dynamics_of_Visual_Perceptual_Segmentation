{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from imageio.v3 import imread, imwrite\n",
    "from PIL import Image\n",
    "import pysaliency\n",
    "from pysaliency.baseline_utils import BaselineModel, CrossvalidatedBaselineModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from deepgaze_pytorch.layers import (\n",
    "    Conv2dMultiInput,\n",
    "    LayerNorm,\n",
    "    LayerNormMultiInput,\n",
    "    Bias,\n",
    "    FlexibleScanpathHistoryEncoding\n",
    ")\n",
    "\n",
    "from deepgaze_pytorch.modules import DeepGazeIII, FeatureExtractor\n",
    "from deepgaze_pytorch.features.densenet import RGBDenseNet201\n",
    "from deepgaze_pytorch.data import ImageDataset, ImageDatasetSampler, FixationDataset, FixationMaskTransform\n",
    "from deepgaze_pytorch.training import _train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_saliency_network(input_channels):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('layernorm0', LayerNorm(input_channels)),\n",
    "        ('conv0', nn.Conv2d(input_channels, 8, (1, 1), bias=False)),\n",
    "        ('bias0', Bias(8)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(8)),\n",
    "        ('conv1', nn.Conv2d(8, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "\n",
    "        ('layernorm2', LayerNorm(16)),\n",
    "        ('conv2', nn.Conv2d(16, 1, (1, 1), bias=False)),\n",
    "        ('bias2', Bias(1)),\n",
    "        ('softplus2', nn.Softplus()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def build_scanpath_network():\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('encoding0', FlexibleScanpathHistoryEncoding(in_fixations=4, channels_per_fixation=3, out_channels=128, kernel_size=[1, 1], bias=True)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(128)),\n",
    "        ('conv1', nn.Conv2d(128, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def build_fixation_selection_network(scanpath_features=16):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('layernorm0', LayerNormMultiInput([1, scanpath_features])),\n",
    "        ('conv0', Conv2dMultiInput([1, scanpath_features], 128, (1, 1), bias=False)),\n",
    "        ('bias0', Bias(128)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(128)),\n",
    "        ('conv1', nn.Conv2d(128, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "\n",
    "        ('conv2', nn.Conv2d(16, 1, (1, 1), bias=False)),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spatial_dataset(stimuli, fixations, centerbias, batch_size, path=None):\n",
    "    if path is not None:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        lmdb_path = str(path)\n",
    "    else:\n",
    "        lmdb_path = None\n",
    "\n",
    "    dataset = ImageDataset(\n",
    "        stimuli=stimuli,\n",
    "        fixations=fixations,\n",
    "        centerbias_model=centerbias,\n",
    "        transform=FixationMaskTransform(sparse=False),\n",
    "        average='image',\n",
    "        lmdb_path=lmdb_path,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=ImageDatasetSampler(dataset, batch_size=batch_size),\n",
    "        pin_memory=True,\n",
    "        num_workers=os.cpu_count(),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_scanpath_dataset(stimuli, fixations, centerbias, batch_size, path=None):\n",
    "    if path is not None:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        lmdb_path = str(path)\n",
    "    else:\n",
    "        lmdb_path = None\n",
    "\n",
    "    dataset = FixationDataset(\n",
    "        stimuli=stimuli,\n",
    "        fixations=fixations,\n",
    "        centerbias_model=centerbias,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "        allow_missing_fixations=True,\n",
    "        transform=FixationMaskTransform(sparse=False),\n",
    "        average='image',\n",
    "        lmdb_path=lmdb_path,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=ImageDatasetSampler(dataset, batch_size=batch_size),\n",
    "        pin_memory=True,\n",
    "        num_workers=os.cpu_count(),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = Path('pysaliency_datasets')\n",
    "train_directory = Path('train_deepgaze3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Using GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on SALICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SALICON data from pysaliency_datasets...\n",
      "SALICON data loaded.\n",
      "Initializing BaselineModel...\n",
      "BaselineModel initialized.\n",
      "Loaded cached train baseline log likelihood from: pysaliency_datasets/salicon_baseline_train_ll.pkl\n",
      "Loaded cached validation baseline log likelihood from: pysaliency_datasets/salicon_baseline_val_ll.pkl\n",
      "------------------------------\n",
      "Final Train Baseline Log Likelihood: 0.46408017115279726\n",
      "Final Validation Baseline Log Likelihood: 0.4291592320821601\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# Assume dataset_directory is defined appropriately, e.g.:\n",
    "# dataset_directory = '/path/to/your/datasets'\n",
    "# Ensure the directory exists or create it if it doesn't\n",
    "# os.makedirs(dataset_directory, exist_ok=True) # Might be needed if dataset_directory is just for caching\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading SALICON data from {dataset_directory}...\")\n",
    "SALICON_train_stimuli, SALICON_train_fixations = pysaliency.get_SALICON_train(location=dataset_directory)\n",
    "SALICON_val_stimuli, SALICON_val_fixations = pysaliency.get_SALICON_val(location=dataset_directory)\n",
    "print(\"SALICON data loaded.\")\n",
    "\n",
    "# --- Define Model ---\n",
    "# parameters taken from an early fit for MIT1003. Since SALICON has many more fixations, the bandwidth won't be too small\n",
    "print(\"Initializing BaselineModel...\")\n",
    "SALICON_centerbias = BaselineModel(stimuli=SALICON_train_stimuli, fixations=SALICON_train_fixations, bandwidth=0.0217, eps=2e-13, caching=False)\n",
    "print(\"BaselineModel initialized.\")\n",
    "\n",
    "# --- Define cache file paths ---\n",
    "train_ll_cache_file = os.path.join(dataset_directory, 'salicon_baseline_train_ll.pkl')\n",
    "val_ll_cache_file = os.path.join(dataset_directory, 'salicon_baseline_val_ll.pkl')\n",
    "\n",
    "# --- Compute or Load Train Baseline Log Likelihood ---\n",
    "try:\n",
    "    # Attempt to load from cache\n",
    "    with open(train_ll_cache_file, 'rb') as f:\n",
    "        train_baseline_log_likelihood = pickle.load(f)\n",
    "    print(f\"Loaded cached train baseline log likelihood from: {train_ll_cache_file}\")\n",
    "except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "    # Compute if cache doesn't exist or is invalid\n",
    "    print(f\"Cache not found or invalid ({e}). Computing train baseline log likelihood...\")\n",
    "    train_baseline_log_likelihood = SALICON_centerbias.information_gain(\n",
    "        SALICON_train_stimuli,\n",
    "        SALICON_train_fixations,\n",
    "        verbose=True,\n",
    "        average='image'\n",
    "    )\n",
    "    print(f\"Computation finished. Train LL = {train_baseline_log_likelihood}\")\n",
    "    # Save the result\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(train_ll_cache_file), exist_ok=True) # Ensure directory exists\n",
    "        with open(train_ll_cache_file, 'wb') as f:\n",
    "            pickle.dump(train_baseline_log_likelihood, f)\n",
    "        print(f\"Saved train baseline log likelihood to: {train_ll_cache_file}\")\n",
    "    except Exception as save_e:\n",
    "        print(f\"Error saving cache file {train_ll_cache_file}: {save_e}\")\n",
    "\n",
    "\n",
    "# --- Compute or Load Validation Baseline Log Likelihood ---\n",
    "try:\n",
    "    # Attempt to load from cache\n",
    "    with open(val_ll_cache_file, 'rb') as f:\n",
    "        val_baseline_log_likelihood = pickle.load(f)\n",
    "    print(f\"Loaded cached validation baseline log likelihood from: {val_ll_cache_file}\")\n",
    "except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "    # Compute if cache doesn't exist or is invalid\n",
    "    print(f\"Cache not found or invalid ({e}). Computing validation baseline log likelihood...\")\n",
    "    val_baseline_log_likelihood = SALICON_centerbias.information_gain(\n",
    "        SALICON_val_stimuli,\n",
    "        SALICON_val_fixations,\n",
    "        verbose=True,\n",
    "        average='image'\n",
    "    )\n",
    "    print(f\"Computation finished. Validation LL = {val_baseline_log_likelihood}\")\n",
    "    # Save the result\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(val_ll_cache_file), exist_ok=True) # Ensure directory exists\n",
    "        with open(val_ll_cache_file, 'wb') as f:\n",
    "            pickle.dump(val_baseline_log_likelihood, f)\n",
    "        print(f\"Saved validation baseline log likelihood to: {val_ll_cache_file}\")\n",
    "    except Exception as save_e:\n",
    "        print(f\"Error saving cache file {val_ll_cache_file}: {save_e}\")\n",
    "\n",
    "\n",
    "# --- Final Output ---\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Train Baseline Log Likelihood: {train_baseline_log_likelihood}\")\n",
    "print(f\"Final Validation Baseline Log Likelihood: {val_baseline_log_likelihood}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mirko/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = DeepGazeIII(\n",
    "    features=FeatureExtractor(RGBDenseNet201(), [\n",
    "            '1.features.denseblock4.denselayer32.norm1',\n",
    "            '1.features.denseblock4.denselayer32.conv1',\n",
    "            '1.features.denseblock4.denselayer31.conv2',\n",
    "        ]),\n",
    "    saliency_network=build_saliency_network(2048),\n",
    "    scanpath_network=None,\n",
    "    fixation_selection_network=build_fixation_selection_network(scanpath_features=0),\n",
    "    downsample=1.5,\n",
    "    readout_factor=4,\n",
    "    saliency_map_factor=4,\n",
    "    included_fixations=[],\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30, 45, 60, 75, 90, 105, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid LMDB found at train_deepgaze3/lmdb_cache/SALICON_train with 10000 items. Skipping generation.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 68992354/68992355 [00:16<00:00, 4282313.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid LMDB found at train_deepgaze3/lmdb_cache/SALICON_val with 5000 items. Skipping generation.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 38846997/38846998 [00:08<00:00, 4367444.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_spatial_dataset(SALICON_train_stimuli, SALICON_train_fixations, SALICON_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / 'SALICON_train')\n",
    "validation_loader = prepare_spatial_dataset(SALICON_val_stimuli, SALICON_val_fixations, SALICON_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / 'SALICON_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Already finished\n"
     ]
    }
   ],
   "source": [
    "_train(train_directory / 'pretraining',\n",
    "    model,\n",
    "    train_loader, train_baseline_log_likelihood,\n",
    "    validation_loader, val_baseline_log_likelihood,\n",
    "    optimizer, lr_scheduler,\n",
    "    minimum_learning_rate=1e-7,\n",
    "    device=device,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the MIT1003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_stimuli_orig, mit_scanpaths_orig = pysaliency.external_datasets.mit.get_mit1003_with_initial_fixation(location=dataset_directory, replace_initial_invalid_fixations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15045/15045 [00:00<00:00, 542180.50it/s]\n",
      "/tmp/ipykernel_384252/2048604840.py:54: UserWarning: don't use attributes for FixationTrains, use scanpath_attributes or scanpath_fixation_attributes instead!\n",
      "  new_fixations = pysaliency.FixationTrains(\n",
      "100%|██████████| 1003/1003 [00:04<00:00, 218.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_stimulus(input_image):\n",
    "    size = input_image.shape[0], input_image.shape[1]\n",
    "    if size[0] < size[1]:\n",
    "        new_size = 768, 1024\n",
    "    else:\n",
    "        new_size = 1024,768\n",
    "    \n",
    "    # pillow uses width, height\n",
    "    new_size = tuple(list(new_size)[::-1])\n",
    "    \n",
    "    new_stimulus = np.array(Image.fromarray(input_image).resize(new_size, Image.BILINEAR))\n",
    "    return new_stimulus\n",
    "\n",
    "def convert_fixations(stimuli, fixations):\n",
    "    new_fixations = fixations.copy()\n",
    "    for n in tqdm(list(range(len(stimuli)))):\n",
    "        stimulus = stimuli.stimuli[n]\n",
    "        size = stimulus.shape[0], stimulus.shape[1]\n",
    "        if size[0] < size[1]:\n",
    "            new_size = 768, 1024\n",
    "        else:\n",
    "            new_size = 1024,768\n",
    "        x_factor = new_size[1] / size[1]\n",
    "        y_factor = new_size[0] / size[0]\n",
    "        \n",
    "        inds = new_fixations.n == n\n",
    "        new_fixations.x[inds] *= x_factor\n",
    "        new_fixations.y[inds] *= y_factor\n",
    "        new_fixations.x_hist[inds] *= x_factor\n",
    "        new_fixations.y_hist[inds] *= y_factor\n",
    "    \n",
    "    return new_fixations\n",
    "\n",
    "def convert_fixation_trains(stimuli, fixations):\n",
    "    train_xs = fixations.train_xs.copy()\n",
    "    train_ys = fixations.train_ys.copy()\n",
    "    \n",
    "    for i in tqdm(range(len(train_xs))):\n",
    "        n = fixations.train_ns[i]\n",
    "        \n",
    "        size = stimuli.shapes[n][0], stimuli.shapes[n][1]\n",
    "        \n",
    "        if size[0] < size[1]:\n",
    "            new_size = 768, 1024\n",
    "        else:\n",
    "            new_size = 1024,768\n",
    "        \n",
    "        x_factor = new_size[1] / size[1]\n",
    "        y_factor = new_size[0] / size[0]\n",
    "        \n",
    "        train_xs[i] *= x_factor\n",
    "        train_ys[i] *= y_factor\n",
    "        \n",
    "    new_fixations = pysaliency.FixationTrains(\n",
    "        train_xs = train_xs,\n",
    "        train_ys = train_ys,\n",
    "        train_ts = fixations.train_ts.copy(),\n",
    "        train_ns = fixations.train_ns.copy(),\n",
    "        train_subjects = fixations.train_subjects.copy(),\n",
    "        attributes={key: getattr(fixations, key).copy() for key in fixations.__attributes__ if key not in ['subjects', 'scanpath_index']},\n",
    "    )\n",
    "    return new_fixations\n",
    "\n",
    "\n",
    "\n",
    "def convert_stimuli(stimuli, new_location: Path):\n",
    "    assert isinstance(stimuli, pysaliency.FileStimuli)\n",
    "    new_stimuli_location = new_location / 'stimuli'\n",
    "    new_stimuli_location.mkdir(parents=True, exist_ok=True)\n",
    "    new_filenames = []\n",
    "    for filename in tqdm(stimuli.filenames):\n",
    "        stimulus = imread(filename)\n",
    "        new_stimulus = convert_stimulus(stimulus)\n",
    "        \n",
    "        basename = os.path.basename(filename)\n",
    "        new_filename = new_stimuli_location / basename\n",
    "        if new_stimulus.size != stimulus.size:\n",
    "            imwrite(new_filename, new_stimulus)\n",
    "        else:\n",
    "            #print(\"Keeping\")\n",
    "            shutil.copy(filename, new_filename)\n",
    "        new_filenames.append(new_filename)\n",
    "    return pysaliency.FileStimuli(new_filenames)\n",
    "\n",
    "mit_scanpaths_twosize = convert_fixation_trains(mit_stimuli_orig, mit_scanpaths_orig)\n",
    "mit_stimuli_twosize = convert_stimuli(mit_stimuli_orig, train_directory / 'MIT1003_twosize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the initial forced fixation from the training data, it's only used for conditioning\n",
    "mit_fixations_twosize = mit_scanpaths_twosize[mit_scanpaths_twosize.lengths > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters optimized on MIT1003 for maximum leave-one-image-out crossvalidation log-likelihood\n",
    "MIT1003_centerbias = CrossvalidatedBaselineModel(\n",
    "    mit_stimuli_twosize,\n",
    "    mit_fixations_twosize,\n",
    "    bandwidth=10**-1.6667673342543432,\n",
    "    eps=10**-14.884189168516073,\n",
    "    caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random shuffles for crossvalidation\n",
      "Using random shuffles for crossvalidation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/808 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [00:55<00:00, 14.46it/s]\n",
      "100%|██████████| 94/94 [00:06<00:00, 15.48it/s]\n",
      "Using cache found in /home/mirko/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB check: Could not open/read existing DB (train_deepgaze3/lmdb_cache/MIT1003_train_spatial_0: No such file or directory). Needs regeneration.\n",
      "Regenerating LMDB at train_deepgaze3/lmdb_cache/MIT1003_train_spatial_0\n",
      "Removing existing path: train_deepgaze3/lmdb_cache/MIT1003_train_spatial_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing LMDB entries: 100%|██████████| 808/808 [00:56<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metadata: count = 808\n",
      "Closing database.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 83717/83718 [00:00<00:00, 4257569.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDB check: Could not open/read existing DB (train_deepgaze3/lmdb_cache/MIT1003_val_spatial_0: No such file or directory). Needs regeneration.\n",
      "Regenerating LMDB at train_deepgaze3/lmdb_cache/MIT1003_val_spatial_0\n",
      "Removing existing path: train_deepgaze3/lmdb_cache/MIT1003_val_spatial_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing LMDB entries: 100%|██████████| 94/94 [00:05<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metadata: count = 94\n",
      "Closing database.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9925/9926 [00:00<00:00, 4287616.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Restoring from train_deepgaze3/pretraining/final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation metrics ['IG', 'LL', 'AUC', 'NSS']\n",
      "Beginning training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating LL 1.87526: 100%|██████████| 25/25 [00:05<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch                  timestamp  learning_rate  loss  validation_IG  \\\n",
      "0      0 2025-04-15 20:59:39.813539          0.001   NaN       0.963517   \n",
      "\n",
      "   validation_LL  validation_AUC  validation_NSS  \n",
      "0       1.875262             0.5        4.517214  \n",
      "validation_LL     0\n",
      "validation_NSS    0\n",
      "validation_AUC    0\n",
      "validation_IG     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-1.90183: 100%|██████████| 202/202 [00:14<00:00, 13.70it/s]\n",
      "Validating LL 2.05253: 100%|██████████| 25/25 [00:02<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch                  timestamp  learning_rate      loss  validation_IG  \\\n",
      "0      0 2025-04-15 20:59:39.813539          0.001       NaN       0.963517   \n",
      "1      1 2025-04-15 20:59:57.706824          0.001 -1.901827       1.140782   \n",
      "\n",
      "   validation_LL  validation_AUC  validation_NSS  \n",
      "0       1.875262             0.5        4.517214  \n",
      "1       2.052527             0.5        6.233057  \n",
      "validation_LL     1\n",
      "validation_NSS    1\n",
      "validation_AUC    0\n",
      "validation_IG     1\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-1.97245: 100%|██████████| 202/202 [00:13<00:00, 15.21it/s]\n",
      "Validating LL 2.06891: 100%|██████████| 25/25 [00:02<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch                  timestamp  learning_rate      loss  validation_IG  \\\n",
      "1      1 2025-04-15 20:59:57.706824          0.001 -1.901827       1.140782   \n",
      "2      2 2025-04-15 21:00:14.092211          0.001 -1.972445       1.157162   \n",
      "\n",
      "   validation_LL  validation_AUC  validation_NSS  \n",
      "1       2.052527             0.5        6.233057  \n",
      "2       2.068907             0.5        6.504853  \n",
      "validation_LL     2\n",
      "validation_NSS    2\n",
      "validation_AUC    0\n",
      "validation_IG     2\n",
      "dtype: int64\n",
      "removing train_deepgaze3/MIT1003_spatial/crossval-10-0/step-0001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-2.00508: 100%|██████████| 202/202 [00:13<00:00, 15.26it/s]\n",
      "Validating LL 2.04020: 100%|██████████| 25/25 [00:02<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch                  timestamp  learning_rate      loss  validation_IG  \\\n",
      "2      2 2025-04-15 21:00:14.092211          0.001 -1.972445       1.157162   \n",
      "3      3 2025-04-15 21:00:30.434525          0.001 -2.005078       1.128459   \n",
      "\n",
      "   validation_LL  validation_AUC  validation_NSS  \n",
      "2       2.068907             0.5        6.504853  \n",
      "3       2.040204             0.5        6.889115  \n",
      "validation_LL     2\n",
      "validation_NSS    3\n",
      "validation_AUC    0\n",
      "validation_IG     2\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-2.05228: 100%|██████████| 202/202 [00:13<00:00, 15.19it/s]\n",
      "Validating LL 2.06916: 100%|██████████| 25/25 [00:02<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch                  timestamp  learning_rate      loss  validation_IG  \\\n",
      "3      3 2025-04-15 21:00:30.434525         0.0010 -2.005078       1.128459   \n",
      "4      4 2025-04-15 21:00:51.166574         0.0001 -2.052282       1.157414   \n",
      "\n",
      "   validation_LL  validation_AUC  validation_NSS  \n",
      "3       2.040204             0.5        6.889115  \n",
      "4       2.069159             0.5        6.409659  \n",
      "validation_LL     4\n",
      "validation_NSS    3\n",
      "validation_AUC    0\n",
      "validation_IG     4\n",
      "dtype: int64\n",
      "removing train_deepgaze3/MIT1003_spatial/crossval-10-0/step-0002.pth\n",
      "removing train_deepgaze3/MIT1003_spatial/crossval-10-0/step-0003.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-2.06108:  96%|█████████▌| 193/202 [00:12<00:00, 15.83it/s]"
     ]
    }
   ],
   "source": [
    "for crossval_fold in range(10):\n",
    "    MIT1003_stimuli_train, MIT1003_fixations_train = pysaliency.dataset_config.train_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=10, fold_no=crossval_fold)\n",
    "    MIT1003_stimuli_val, MIT1003_fixations_val = pysaliency.dataset_config.validation_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=10, fold_no=crossval_fold)\n",
    "\n",
    "    train_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_train, MIT1003_fixations_train, verbose=True, average='image')\n",
    "    val_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_val, MIT1003_fixations_val, verbose=True, average='image')\n",
    "\n",
    "    # finetune spatial model on MIT1003\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=None,\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=0),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[],\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18, 21, 24])\n",
    "\n",
    "    train_loader = prepare_spatial_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_train_spatial_{crossval_fold}')\n",
    "    validation_loader = prepare_spatial_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_val_spatial_{crossval_fold}')\n",
    "\n",
    "    _train(train_directory / 'MIT1003_spatial' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'pretraining' / 'final.pth',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Train scanpath model\n",
    "\n",
    "    train_loader = prepare_scanpath_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_train_scanpath_{crossval_fold}')\n",
    "    validation_loader = prepare_scanpath_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_val_scanpath_{crossval_fold}')\n",
    "\n",
    "    # first train with partially frozen saliency network\n",
    "\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=build_scanpath_network(),\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=16),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    frozen_scopes = [\n",
    "        \"saliency_network.layernorm0\",\n",
    "        \"saliency_network.conv0\",\n",
    "        \"saliency_network.bias0\",\n",
    "        \"saliency_network.layernorm1\",\n",
    "        \"saliency_network.conv1\",\n",
    "        \"saliency_network.bias1\",\n",
    "    ]\n",
    "\n",
    "    for scope in frozen_scopes:\n",
    "        for parameter_name, parameter in model.named_parameters():\n",
    "            if parameter_name.startswith(scope):\n",
    "                print(\"Fixating parameter\", parameter_name)\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 31, 32, 33, 34, 35])\n",
    "\n",
    "    _train(train_directory / 'MIT1003_scanpath_partially_frozen_saliency_network' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'MIT1003_spatial' /  f'crossval-10-{crossval_fold}' / 'final.pth'\n",
    "    )\n",
    "\n",
    "    # Now finetune full scanpath model\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=build_scanpath_network(),\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=16),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18, 21, 24])\n",
    "\n",
    "    _train(train_directory / 'MIT1003_scanpath' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'MIT1003_scanpath_partially_frozen_saliency_network' / f'crossval-10-{crossval_fold}' / 'final.pth'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
