{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from imageio.v3 import imread, imwrite\n",
    "from PIL import Image\n",
    "import pysaliency\n",
    "from pysaliency.baseline_utils import BaselineModel, CrossvalidatedBaselineModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from deepgaze_pytorch.layers import (\n",
    "    Conv2dMultiInput,\n",
    "    LayerNorm,\n",
    "    LayerNormMultiInput,\n",
    "    Bias,\n",
    "    FlexibleScanpathHistoryEncoding\n",
    ")\n",
    "\n",
    "from deepgaze_pytorch.modules import DeepGazeIII, FeatureExtractor\n",
    "from deepgaze_pytorch.features.densenet import RGBDenseNet201\n",
    "from deepgaze_pytorch.data import ImageDataset, ImageDatasetSampler, FixationDataset, FixationMaskTransform\n",
    "from deepgaze_pytorch.training import _train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_saliency_network(input_channels):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('layernorm0', LayerNorm(input_channels)),\n",
    "        ('conv0', nn.Conv2d(input_channels, 8, (1, 1), bias=False)),\n",
    "        ('bias0', Bias(8)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(8)),\n",
    "        ('conv1', nn.Conv2d(8, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "\n",
    "        ('layernorm2', LayerNorm(16)),\n",
    "        ('conv2', nn.Conv2d(16, 1, (1, 1), bias=False)),\n",
    "        ('bias2', Bias(1)),\n",
    "        ('softplus2', nn.Softplus()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def build_scanpath_network():\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('encoding0', FlexibleScanpathHistoryEncoding(in_fixations=4, channels_per_fixation=3, out_channels=128, kernel_size=[1, 1], bias=True)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(128)),\n",
    "        ('conv1', nn.Conv2d(128, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def build_fixation_selection_network(scanpath_features=16):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('layernorm0', LayerNormMultiInput([1, scanpath_features])),\n",
    "        ('conv0', Conv2dMultiInput([1, scanpath_features], 128, (1, 1), bias=False)),\n",
    "        ('bias0', Bias(128)),\n",
    "        ('softplus0', nn.Softplus()),\n",
    "\n",
    "        ('layernorm1', LayerNorm(128)),\n",
    "        ('conv1', nn.Conv2d(128, 16, (1, 1), bias=False)),\n",
    "        ('bias1', Bias(16)),\n",
    "        ('softplus1', nn.Softplus()),\n",
    "\n",
    "        ('conv2', nn.Conv2d(16, 1, (1, 1), bias=False)),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spatial_dataset(stimuli, fixations, centerbias, batch_size, path=None):\n",
    "    if path is not None:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        lmdb_path = str(path)\n",
    "    else:\n",
    "        lmdb_path = None\n",
    "\n",
    "    dataset = ImageDataset(\n",
    "        stimuli=stimuli,\n",
    "        fixations=fixations,\n",
    "        centerbias_model=centerbias,\n",
    "        transform=FixationMaskTransform(sparse=False),\n",
    "        average='image',\n",
    "        lmdb_path=lmdb_path,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=ImageDatasetSampler(dataset, batch_size=batch_size),\n",
    "        pin_memory=True,\n",
    "        num_workers=os.cpu_count(),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_scanpath_dataset(stimuli, fixations, centerbias, batch_size, path=None):\n",
    "    if path is not None:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        lmdb_path = str(path)\n",
    "    else:\n",
    "        lmdb_path = None\n",
    "\n",
    "    dataset = FixationDataset(\n",
    "        stimuli=stimuli,\n",
    "        fixations=fixations,\n",
    "        centerbias_model=centerbias,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "        allow_missing_fixations=True,\n",
    "        transform=FixationMaskTransform(sparse=False),\n",
    "        average='image',\n",
    "        lmdb_path=lmdb_path,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=ImageDatasetSampler(dataset, batch_size=batch_size),\n",
    "        pin_memory=True,\n",
    "        num_workers=os.cpu_count(),\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = Path('pysaliency_datasets')\n",
    "train_directory = Path('train_deepgaze3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Using GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on SALICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SALICON data from pysaliency_datasets...\n",
      "SALICON data loaded.\n",
      "Initializing BaselineModel...\n",
      "BaselineModel initialized.\n",
      "Loaded cached train baseline log likelihood from: pysaliency_datasets/salicon_baseline_train_ll.pkl\n",
      "Loaded cached validation baseline log likelihood from: pysaliency_datasets/salicon_baseline_val_ll.pkl\n",
      "------------------------------\n",
      "Final Train Baseline Log Likelihood: 0.46408017115279726\n",
      "Final Validation Baseline Log Likelihood: 0.4291592320821601\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# Assume dataset_directory is defined appropriately, e.g.:\n",
    "# dataset_directory = '/path/to/your/datasets'\n",
    "# Ensure the directory exists or create it if it doesn't\n",
    "# os.makedirs(dataset_directory, exist_ok=True) # Might be needed if dataset_directory is just for caching\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading SALICON data from {dataset_directory}...\")\n",
    "SALICON_train_stimuli, SALICON_train_fixations = pysaliency.get_SALICON_train(location=dataset_directory)\n",
    "SALICON_val_stimuli, SALICON_val_fixations = pysaliency.get_SALICON_val(location=dataset_directory)\n",
    "print(\"SALICON data loaded.\")\n",
    "\n",
    "# --- Define Model ---\n",
    "# parameters taken from an early fit for MIT1003. Since SALICON has many more fixations, the bandwidth won't be too small\n",
    "print(\"Initializing BaselineModel...\")\n",
    "SALICON_centerbias = BaselineModel(stimuli=SALICON_train_stimuli, fixations=SALICON_train_fixations, bandwidth=0.0217, eps=2e-13, caching=False)\n",
    "print(\"BaselineModel initialized.\")\n",
    "\n",
    "# --- Define cache file paths ---\n",
    "train_ll_cache_file = os.path.join(dataset_directory, 'salicon_baseline_train_ll.pkl')\n",
    "val_ll_cache_file = os.path.join(dataset_directory, 'salicon_baseline_val_ll.pkl')\n",
    "\n",
    "# --- Compute or Load Train Baseline Log Likelihood ---\n",
    "try:\n",
    "    # Attempt to load from cache\n",
    "    with open(train_ll_cache_file, 'rb') as f:\n",
    "        train_baseline_log_likelihood = pickle.load(f)\n",
    "    print(f\"Loaded cached train baseline log likelihood from: {train_ll_cache_file}\")\n",
    "except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "    # Compute if cache doesn't exist or is invalid\n",
    "    print(f\"Cache not found or invalid ({e}). Computing train baseline log likelihood...\")\n",
    "    train_baseline_log_likelihood = SALICON_centerbias.information_gain(\n",
    "        SALICON_train_stimuli,\n",
    "        SALICON_train_fixations,\n",
    "        verbose=True,\n",
    "        average='image'\n",
    "    )\n",
    "    print(f\"Computation finished. Train LL = {train_baseline_log_likelihood}\")\n",
    "    # Save the result\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(train_ll_cache_file), exist_ok=True) # Ensure directory exists\n",
    "        with open(train_ll_cache_file, 'wb') as f:\n",
    "            pickle.dump(train_baseline_log_likelihood, f)\n",
    "        print(f\"Saved train baseline log likelihood to: {train_ll_cache_file}\")\n",
    "    except Exception as save_e:\n",
    "        print(f\"Error saving cache file {train_ll_cache_file}: {save_e}\")\n",
    "\n",
    "\n",
    "# --- Compute or Load Validation Baseline Log Likelihood ---\n",
    "try:\n",
    "    # Attempt to load from cache\n",
    "    with open(val_ll_cache_file, 'rb') as f:\n",
    "        val_baseline_log_likelihood = pickle.load(f)\n",
    "    print(f\"Loaded cached validation baseline log likelihood from: {val_ll_cache_file}\")\n",
    "except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "    # Compute if cache doesn't exist or is invalid\n",
    "    print(f\"Cache not found or invalid ({e}). Computing validation baseline log likelihood...\")\n",
    "    val_baseline_log_likelihood = SALICON_centerbias.information_gain(\n",
    "        SALICON_val_stimuli,\n",
    "        SALICON_val_fixations,\n",
    "        verbose=True,\n",
    "        average='image'\n",
    "    )\n",
    "    print(f\"Computation finished. Validation LL = {val_baseline_log_likelihood}\")\n",
    "    # Save the result\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(val_ll_cache_file), exist_ok=True) # Ensure directory exists\n",
    "        with open(val_ll_cache_file, 'wb') as f:\n",
    "            pickle.dump(val_baseline_log_likelihood, f)\n",
    "        print(f\"Saved validation baseline log likelihood to: {val_ll_cache_file}\")\n",
    "    except Exception as save_e:\n",
    "        print(f\"Error saving cache file {val_ll_cache_file}: {save_e}\")\n",
    "\n",
    "\n",
    "# --- Final Output ---\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Train Baseline Log Likelihood: {train_baseline_log_likelihood}\")\n",
    "print(f\"Final Validation Baseline Log Likelihood: {val_baseline_log_likelihood}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mirko/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mirko/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = DeepGazeIII(\n",
    "    features=FeatureExtractor(RGBDenseNet201(), [\n",
    "            '1.features.denseblock4.denselayer32.norm1',\n",
    "            '1.features.denseblock4.denselayer32.conv1',\n",
    "            '1.features.denseblock4.denselayer31.conv2',\n",
    "        ]),\n",
    "    saliency_network=build_saliency_network(2048),\n",
    "    scanpath_network=None,\n",
    "    fixation_selection_network=build_fixation_selection_network(scanpath_features=0),\n",
    "    downsample=1.5,\n",
    "    readout_factor=4,\n",
    "    saliency_map_factor=4,\n",
    "    included_fixations=[],\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30, 45, 60, 75, 90, 105, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid LMDB found at train_deepgaze3/lmdb_cache/SALICON_train with 10000 items. Skipping generation.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 68992354/68992355 [00:16<00:00, 4251581.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid LMDB found at train_deepgaze3/lmdb_cache/SALICON_val with 5000 items. Skipping generation.\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 38846997/38846998 [00:08<00:00, 4420474.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader = prepare_spatial_dataset(SALICON_train_stimuli, SALICON_train_fixations, SALICON_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / 'SALICON_train')\n",
    "validation_loader = prepare_spatial_dataset(SALICON_val_stimuli, SALICON_val_fixations, SALICON_centerbias, batch_size=32, path=train_directory / 'lmdb_cache' / 'SALICON_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "validation metrics ['IG', 'LL', 'AUC', 'NSS']\n",
      "Found old checkpoint train_deepgaze3/pretraining/step-0016.pth\n",
      "Restoring from train_deepgaze3/pretraining/step-0016.pth\n",
      "Setting step to 16\n",
      "Continuing from step 16\n",
      "Epoch not yet evaluated, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating LL 0.79286: 100%|██████████| 157/157 [00:53<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch                   timestamp  learning_rate      loss  validation_IG  \\\n",
      "15     15  2025-04-14 20:54:18.132296         0.0010 -0.960792       0.361880   \n",
      "16     16  2025-04-14 21:16:30.415608         0.0001 -0.966189       0.363696   \n",
      "\n",
      "    validation_LL  validation_AUC  validation_NSS  \n",
      "15       0.791039        0.769591        1.516718  \n",
      "16       0.792855        0.500000        1.513873  \n",
      "validation_IG     14\n",
      "validation_LL     14\n",
      "validation_AUC    15\n",
      "validation_NSS    11\n",
      "dtype: int64\n",
      "removing train_deepgaze3/pretraining/step-0015.pth\n",
      "    epoch                   timestamp  learning_rate      loss  validation_IG  \\\n",
      "0       0  2025-04-14 15:25:26.326971         0.0010       NaN       0.002576   \n",
      "1       1  2025-04-14 15:29:12.775332         0.0010 -0.887987       0.338263   \n",
      "2       2  2025-04-14 15:32:57.820775         0.0010 -0.923786       0.342327   \n",
      "3       3  2025-04-14 15:36:42.424298         0.0010 -0.932136       0.351575   \n",
      "4       4  2025-04-14 15:40:27.035175         0.0010 -0.937198       0.347356   \n",
      "5       5  2025-04-14 15:44:11.366974         0.0010 -0.941545       0.343749   \n",
      "6       6  2025-04-14 15:47:57.353423         0.0010 -0.945351       0.354615   \n",
      "7       7  2025-04-14 15:51:44.643524         0.0010 -0.948712       0.354580   \n",
      "8       8  2025-04-14 15:56:35.803111         0.0010 -0.951515       0.356971   \n",
      "9       9  2025-04-14 16:00:28.799297         0.0010 -0.954283       0.357303   \n",
      "10     10  2025-04-14 16:04:23.200176         0.0010 -0.955662       0.358435   \n",
      "11     11  2025-04-14 16:08:30.663507         0.0010 -0.957062       0.355484   \n",
      "12     12  2025-04-14 20:43:32.587687         0.0010 -0.958281       0.360837   \n",
      "13     13  2025-04-14 20:47:06.774625         0.0010 -0.959659       0.354402   \n",
      "14     14  2025-04-14 20:50:42.676982         0.0010 -0.959819       0.364467   \n",
      "15     15  2025-04-14 20:54:18.132296         0.0010 -0.960792       0.361880   \n",
      "16     16  2025-04-14 21:16:30.415608         0.0001 -0.966189       0.363696   \n",
      "\n",
      "    validation_LL  validation_AUC  validation_NSS  \n",
      "0        0.431735        0.709627        0.836227  \n",
      "1        0.767422        0.766931        1.400089  \n",
      "2        0.771486        0.767440        1.474475  \n",
      "3        0.780735        0.768141        1.452549  \n",
      "4        0.776516        0.767934        1.469161  \n",
      "5        0.772908        0.768332        1.518961  \n",
      "6        0.783774        0.768513        1.466179  \n",
      "7        0.783739        0.768668        1.506446  \n",
      "8        0.786130        0.768995        1.484012  \n",
      "9        0.786462        0.768978        1.508956  \n",
      "10       0.787595        0.769237        1.506007  \n",
      "11       0.784643        0.769199        1.533222  \n",
      "12       0.789996        0.769282        1.507735  \n",
      "13       0.783561        0.768748        1.522096  \n",
      "14       0.793627        0.769292        1.462064  \n",
      "15       0.791039        0.769591        1.516718  \n",
      "16       0.792855        0.500000        1.513873  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-0.96708: 100%|██████████| 313/313 [00:55<00:00,  5.66it/s]\n",
      "Validating LL 0.78840: 100%|██████████| 157/157 [00:50<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch                   timestamp  learning_rate      loss  validation_IG  \\\n",
      "16     16  2025-04-14 21:16:30.415608         0.0001 -0.966189       0.363696   \n",
      "17     17  2025-04-14 21:18:16.828836         0.0001 -0.967082       0.359242   \n",
      "\n",
      "    validation_LL  validation_AUC  validation_NSS  \n",
      "16       0.792855             0.5        1.513873  \n",
      "17       0.788401             0.5        1.529752  \n",
      "validation_IG     14\n",
      "validation_LL     14\n",
      "validation_AUC    15\n",
      "validation_NSS    11\n",
      "dtype: int64\n",
      "removing train_deepgaze3/pretraining/step-0016.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-0.96725: 100%|██████████| 313/313 [00:54<00:00,  5.79it/s]\n",
      "Validating LL 0.79111: 100%|██████████| 157/157 [00:50<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch                   timestamp  learning_rate      loss  validation_IG  \\\n",
      "17     17  2025-04-14 21:18:16.828836         0.0001 -0.967082       0.359242   \n",
      "18     18  2025-04-14 21:20:01.973876         0.0001 -0.967253       0.361947   \n",
      "\n",
      "    validation_LL  validation_AUC  validation_NSS  \n",
      "17       0.788401             0.5        1.529752  \n",
      "18       0.791106             0.5        1.521176  \n",
      "validation_IG     14\n",
      "validation_LL     14\n",
      "validation_AUC    15\n",
      "validation_NSS    11\n",
      "dtype: int64\n",
      "removing train_deepgaze3/pretraining/step-0017.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-0.96478:  84%|████████▎ | 262/313 [00:45<00:08,  5.86it/s]"
     ]
    }
   ],
   "source": [
    "_train(train_directory / 'pretraining',\n",
    "    model,\n",
    "    train_loader, train_baseline_log_likelihood,\n",
    "    validation_loader, val_baseline_log_likelihood,\n",
    "    optimizer, lr_scheduler,\n",
    "    minimum_learning_rate=1e-7,\n",
    "    device=device,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the MIT1003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mit_stimuli_orig, mit_scanpaths_orig = pysaliency.external_datasets.mit.get_mit1003_with_initial_fixation(location=dataset_directory, replace_initial_invalid_fixations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15045/15045 [00:00<00:00, 172014.85it/s]\n",
      "/tmp/ipykernel_822328/2048604840.py:54: UserWarning: don't use attributes for FixationTrains, use scanpath_attributes or scanpath_fixation_attributes instead!\n",
      "  new_fixations = pysaliency.FixationTrains(\n",
      "100%|██████████| 1003/1003 [00:17<00:00, 57.52it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def convert_stimulus(input_image):\n",
    "    size = input_image.shape[0], input_image.shape[1]\n",
    "    if size[0] < size[1]:\n",
    "        new_size = 768, 1024\n",
    "    else:\n",
    "        new_size = 1024,768\n",
    "    \n",
    "    # pillow uses width, height\n",
    "    new_size = tuple(list(new_size)[::-1])\n",
    "    \n",
    "    new_stimulus = np.array(Image.fromarray(input_image).resize(new_size, Image.BILINEAR))\n",
    "    return new_stimulus\n",
    "\n",
    "def convert_fixations(stimuli, fixations):\n",
    "    new_fixations = fixations.copy()\n",
    "    for n in tqdm(list(range(len(stimuli)))):\n",
    "        stimulus = stimuli.stimuli[n]\n",
    "        size = stimulus.shape[0], stimulus.shape[1]\n",
    "        if size[0] < size[1]:\n",
    "            new_size = 768, 1024\n",
    "        else:\n",
    "            new_size = 1024,768\n",
    "        x_factor = new_size[1] / size[1]\n",
    "        y_factor = new_size[0] / size[0]\n",
    "        \n",
    "        inds = new_fixations.n == n\n",
    "        new_fixations.x[inds] *= x_factor\n",
    "        new_fixations.y[inds] *= y_factor\n",
    "        new_fixations.x_hist[inds] *= x_factor\n",
    "        new_fixations.y_hist[inds] *= y_factor\n",
    "    \n",
    "    return new_fixations\n",
    "\n",
    "def convert_fixation_trains(stimuli, fixations):\n",
    "    train_xs = fixations.train_xs.copy()\n",
    "    train_ys = fixations.train_ys.copy()\n",
    "    \n",
    "    for i in tqdm(range(len(train_xs))):\n",
    "        n = fixations.train_ns[i]\n",
    "        \n",
    "        size = stimuli.shapes[n][0], stimuli.shapes[n][1]\n",
    "        \n",
    "        if size[0] < size[1]:\n",
    "            new_size = 768, 1024\n",
    "        else:\n",
    "            new_size = 1024,768\n",
    "        \n",
    "        x_factor = new_size[1] / size[1]\n",
    "        y_factor = new_size[0] / size[0]\n",
    "        \n",
    "        train_xs[i] *= x_factor\n",
    "        train_ys[i] *= y_factor\n",
    "        \n",
    "    new_fixations = pysaliency.FixationTrains(\n",
    "        train_xs = train_xs,\n",
    "        train_ys = train_ys,\n",
    "        train_ts = fixations.train_ts.copy(),\n",
    "        train_ns = fixations.train_ns.copy(),\n",
    "        train_subjects = fixations.train_subjects.copy(),\n",
    "        attributes={key: getattr(fixations, key).copy() for key in fixations.__attributes__ if key not in ['subjects', 'scanpath_index']},\n",
    "    )\n",
    "    return new_fixations\n",
    "\n",
    "\n",
    "\n",
    "def convert_stimuli(stimuli, new_location: Path):\n",
    "    assert isinstance(stimuli, pysaliency.FileStimuli)\n",
    "    new_stimuli_location = new_location / 'stimuli'\n",
    "    new_stimuli_location.mkdir(parents=True, exist_ok=True)\n",
    "    new_filenames = []\n",
    "    for filename in tqdm(stimuli.filenames):\n",
    "        stimulus = imread(filename)\n",
    "        new_stimulus = convert_stimulus(stimulus)\n",
    "        \n",
    "        basename = os.path.basename(filename)\n",
    "        new_filename = new_stimuli_location / basename\n",
    "        if new_stimulus.size != stimulus.size:\n",
    "            imwrite(new_filename, new_stimulus)\n",
    "        else:\n",
    "            #print(\"Keeping\")\n",
    "            shutil.copy(filename, new_filename)\n",
    "        new_filenames.append(new_filename)\n",
    "    return pysaliency.FileStimuli(new_filenames)\n",
    "\n",
    "mit_scanpaths_twosize = convert_fixation_trains(mit_stimuli_orig, mit_scanpaths_orig)\n",
    "mit_stimuli_twosize = convert_stimuli(mit_stimuli_orig, train_directory / 'MIT1003_twosize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# remove the initial forced fixation from the training data, it's only used for conditioning\n",
    "mit_fixations_twosize = mit_scanpaths_twosize[mit_scanpaths_twosize.lengths > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# parameters optimized on MIT1003 for maximum leave-one-image-out crossvalidation log-likelihood\n",
    "MIT1003_centerbias = CrossvalidatedBaselineModel(\n",
    "    mit_stimuli_twosize,\n",
    "    mit_fixations_twosize,\n",
    "    bandwidth=10**-1.6667673342543432,\n",
    "    eps=10**-14.884189168516073,\n",
    "    caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random shuffles for crossvalidation\n",
      "Using random shuffles for crossvalidation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [02:27<00:00,  5.49it/s]\n",
      "100%|██████████| 94/94 [00:16<00:00,  5.84it/s]\n",
      "Using cache found in /home/mmorello/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/home/mmorello/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mmorello/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate LMDB to train_deepgaze3/lmdb_cache/MIT1003_train_spatial_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808/808 [03:04<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing database ...\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 83717/83718 [00:00<00:00, 819458.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate LMDB to train_deepgaze3/lmdb_cache/MIT1003_val_spatial_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:15<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing database ...\n",
      "Populating fixations cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9925/9926 [00:00<00:00, 600093.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Restoring from train_deepgaze3/pretraining/final.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_deepgaze3/pretraining/final.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m train_loader = prepare_spatial_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=\u001b[32m4\u001b[39m, path=train_directory / \u001b[33m'\u001b[39m\u001b[33mlmdb_cache\u001b[39m\u001b[33m'\u001b[39m / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMIT1003_train_spatial_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrossval_fold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m validation_loader = prepare_spatial_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=\u001b[32m4\u001b[39m, path=train_directory / \u001b[33m'\u001b[39m\u001b[33mlmdb_cache\u001b[39m\u001b[33m'\u001b[39m / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMIT1003_val_spatial_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrossval_fold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMIT1003_spatial\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcrossval-10-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcrossval_fold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_baseline_log_likelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_baseline_log_likelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminimum_learning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartwith\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpretraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfinal.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Train scanpath model\u001b[39;00m\n\u001b[32m     45\u001b[39m train_loader = prepare_scanpath_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=\u001b[32m4\u001b[39m, path=train_directory / \u001b[33m'\u001b[39m\u001b[33mlmdb_cache\u001b[39m\u001b[33m'\u001b[39m / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMIT1003_train_scanpath_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrossval_fold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/DeepGaze/deepgaze_pytorch/training.py:193\u001b[39m, in \u001b[36m_train\u001b[39m\u001b[34m(this_directory, model, train_loader, train_baseline_log_likelihood, val_loader, val_baseline_log_likelihood, optimizer, lr_scheduler, minimum_learning_rate, validation_metric, validation_metrics, validation_epochs, startwith, device)\u001b[39m\n\u001b[32m    190\u001b[39m val_metrics = defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: [])\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m startwith \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[43mrestore_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartwith\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m writer = SummaryWriter(os.path.join(this_directory, \u001b[33m'\u001b[39m\u001b[33mlog\u001b[39m\u001b[33m'\u001b[39m), flush_secs=\u001b[32m30\u001b[39m)\n\u001b[32m    197\u001b[39m columns = [\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/DeepGaze/deepgaze_pytorch/training.py:130\u001b[39m, in \u001b[36mrestore_from_checkpoint\u001b[39m\u001b[34m(model, optimizer, scheduler, path)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrestore_from_checkpoint\u001b[39m(model, optimizer, scheduler, path):\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRestoring from\u001b[39m\u001b[33m\"\u001b[39m, path)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m    132\u001b[39m         \u001b[38;5;66;03m# checkpoint contains training progress\u001b[39;00m\n\u001b[32m    133\u001b[39m         model.load_state_dict(data[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Decoding_Neural_Dynamics_of_Visual_Perceptual_Segmentation/.pixi/envs/default/lib/python3.12/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train_deepgaze3/pretraining/final.pth'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for crossval_fold in range(10):\n",
    "    MIT1003_stimuli_train, MIT1003_fixations_train = pysaliency.dataset_config.train_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=10, fold_no=crossval_fold)\n",
    "    MIT1003_stimuli_val, MIT1003_fixations_val = pysaliency.dataset_config.validation_split(mit_stimuli_twosize, mit_fixations_twosize, crossval_folds=10, fold_no=crossval_fold)\n",
    "\n",
    "    train_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_train, MIT1003_fixations_train, verbose=True, average='image')\n",
    "    val_baseline_log_likelihood = MIT1003_centerbias.information_gain(MIT1003_stimuli_val, MIT1003_fixations_val, verbose=True, average='image')\n",
    "\n",
    "    # finetune spatial model on MIT1003\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=None,\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=0),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[],\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18, 21, 24])\n",
    "\n",
    "    train_loader = prepare_spatial_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_train_spatial_{crossval_fold}')\n",
    "    validation_loader = prepare_spatial_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_val_spatial_{crossval_fold}')\n",
    "\n",
    "    _train(train_directory / 'MIT1003_spatial' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'pretraining' / 'final.pth',\n",
    "    )\n",
    "\n",
    "\n",
    "    # Train scanpath model\n",
    "\n",
    "    train_loader = prepare_scanpath_dataset(MIT1003_stimuli_train, MIT1003_fixations_train, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_train_scanpath_{crossval_fold}')\n",
    "    validation_loader = prepare_scanpath_dataset(MIT1003_stimuli_val, MIT1003_fixations_val, MIT1003_centerbias, batch_size=4, path=train_directory / 'lmdb_cache' / f'MIT1003_val_scanpath_{crossval_fold}')\n",
    "\n",
    "    # first train with partially frozen saliency network\n",
    "\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=build_scanpath_network(),\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=16),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    frozen_scopes = [\n",
    "        \"saliency_network.layernorm0\",\n",
    "        \"saliency_network.conv0\",\n",
    "        \"saliency_network.bias0\",\n",
    "        \"saliency_network.layernorm1\",\n",
    "        \"saliency_network.conv1\",\n",
    "        \"saliency_network.bias1\",\n",
    "    ]\n",
    "\n",
    "    for scope in frozen_scopes:\n",
    "        for parameter_name, parameter in model.named_parameters():\n",
    "            if parameter_name.startswith(scope):\n",
    "                print(\"Fixating parameter\", parameter_name)\n",
    "                parameter.requires_grad = False\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30, 31, 32, 33, 34, 35])\n",
    "\n",
    "    _train(train_directory / 'MIT1003_scanpath_partially_frozen_saliency_network' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'MIT1003_spatial' /  f'crossval-10-{crossval_fold}' / 'final.pth'\n",
    "    )\n",
    "\n",
    "    # Now finetune full scanpath model\n",
    "\n",
    "    model = DeepGazeIII(\n",
    "        features=FeatureExtractor(RGBDenseNet201(), [\n",
    "                '1.features.denseblock4.denselayer32.norm1',\n",
    "                '1.features.denseblock4.denselayer32.conv1',\n",
    "                '1.features.denseblock4.denselayer31.conv2',\n",
    "            ]),\n",
    "        saliency_network=build_saliency_network(2048),\n",
    "        scanpath_network=build_scanpath_network(),\n",
    "        fixation_selection_network=build_fixation_selection_network(scanpath_features=16),\n",
    "        downsample=2,\n",
    "        readout_factor=4,\n",
    "        saliency_map_factor=4,\n",
    "        included_fixations=[-1, -2, -3, -4],\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18, 21, 24])\n",
    "\n",
    "    _train(train_directory / 'MIT1003_scanpath' / f'crossval-10-{crossval_fold}',\n",
    "        model,\n",
    "        train_loader, train_baseline_log_likelihood,\n",
    "        validation_loader, val_baseline_log_likelihood,\n",
    "        optimizer, lr_scheduler,\n",
    "        minimum_learning_rate=1e-7,\n",
    "        device=device,\n",
    "        startwith=train_directory / 'MIT1003_scanpath_partially_frozen_saliency_network' / f'crossval-10-{crossval_fold}' / 'final.pth'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
